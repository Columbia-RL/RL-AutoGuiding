{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from constants import TASK_LIST\n",
    "from constants import INITIAL_ALPHA\n",
    "from constants import INITIAL_GAMMA\n",
    "from constants import EPSILON\n",
    "from constants import NUM_TRA_EPISODES_DQN\n",
    "from constants import NUM_EVAL_EPISODES\n",
    "from scene_loader import THORDiscreteEnvironment as Environment\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "\n",
    "# Deep Q Network off-policy\n",
    "class DeepQNetwork:\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            learning_rate=0.01,\n",
    "            reward_decay=0.9,\n",
    "            e_greedy=0.9,\n",
    "            replace_target_iter=300,\n",
    "            memory_size=500,\n",
    "            batch_size=32,\n",
    "            e_greedy_increment=None,\n",
    "            output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "\n",
    "        # total learning step\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # initialize zero memory [s, a, r, s_]\n",
    "        self.memory = np.zeros((self.memory_size, n_features * 2 + 2))\n",
    "\n",
    "        # consist of [target_net, evaluate_net]\n",
    "        self._build_net()\n",
    "        t_params = tf.get_collection('target_net_params')\n",
    "        e_params = tf.get_collection('eval_net_params')\n",
    "        self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        if output_graph:\n",
    "            # $ tensorboard --logdir=logs\n",
    "            # tf.train.SummaryWriter soon be deprecated, use following\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.cost_his = []\n",
    "\n",
    "    def _build_net(self):\n",
    "        # ------------------ build evaluate_net ------------------\n",
    "        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # input\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target')  # for calculating loss\n",
    "        with tf.variable_scope('eval_net'):\n",
    "            # c_names(collections_names) are the collections to store variables\n",
    "            c_names, n_l1, w_initializer, b_initializer = \\\n",
    "                ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 512, \\\n",
    "                tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)  # config of layers\n",
    "\n",
    "            # first layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)\n",
    "\n",
    "            # second layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_eval = tf.matmul(l1, w2) + b2\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "        with tf.variable_scope('train'):\n",
    "            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        # ------------------ build target_net ------------------\n",
    "        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')    # input\n",
    "        with tf.variable_scope('target_net'):\n",
    "            # c_names(collections_names) are the collections to store variables\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "            # first layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)\n",
    "\n",
    "            # second layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_next = tf.matmul(l1, w2) + b2\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[index, :] = transition\n",
    "\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # to have batch dimension when feed into tf placeholder\n",
    "        observation = observation[np.newaxis, :]\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # forward feed the observation and get q value for every actions\n",
    "            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # check to replace target parameters\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.sess.run(self.replace_target_op)\n",
    "            #print('\\ntarget_params_replaced\\n')\n",
    "\n",
    "        # sample batch memory from all memory\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        q_next, q_eval = self.sess.run(\n",
    "            [self.q_next, self.q_eval],\n",
    "            feed_dict={\n",
    "                self.s_: batch_memory[:, -self.n_features:],  # fixed params\n",
    "                self.s: batch_memory[:, :self.n_features],  # newest params\n",
    "            })\n",
    "\n",
    "        # change q_target w.r.t q_eval's action\n",
    "        q_target = q_eval.copy()\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.n_features].astype(int)\n",
    "        reward = batch_memory[:, self.n_features + 1]\n",
    "\n",
    "        q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "\n",
    "        \"\"\"\n",
    "        For example in this batch I have 2 samples and 3 actions:\n",
    "        q_eval =\n",
    "        [[1, 2, 3],\n",
    "         [4, 5, 6]]\n",
    "\n",
    "        q_target = q_eval =\n",
    "        [[1, 2, 3],\n",
    "         [4, 5, 6]]\n",
    "\n",
    "        Then change q_target with the real q_target value w.r.t the q_eval's action.\n",
    "        For example in:\n",
    "            sample 0, I took action 0, and the max q_target value is -1;\n",
    "            sample 1, I took action 2, and the max q_target value is -2:\n",
    "        q_target =\n",
    "        [[-1, 2, 3],\n",
    "         [4, 5, -2]]\n",
    "\n",
    "        So the (q_target - q_eval) becomes:\n",
    "        [[(-1)-(1), 0, 0],\n",
    "         [0, 0, (-2)-(6)]]\n",
    "\n",
    "        We then backpropagate this error w.r.t the corresponding action to network,\n",
    "        leave other action as error=0 cause we didn't choose it.\n",
    "        \"\"\"\n",
    "\n",
    "        # train eval network\n",
    "        _, self.cost = self.sess.run([self._train_op, self.loss],\n",
    "                                     feed_dict={self.s: batch_memory[:, :self.n_features],\n",
    "                                                self.q_target: q_target})\n",
    "        self.cost_his.append(self.cost)\n",
    "\n",
    "        # increasing epsilon\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "    def plot_cost(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(np.arange(len(self.cost_his)), self.cost_his)\n",
    "        plt.ylim(0,10)\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('training steps')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(compare_list, env, RL):\n",
    "    reward_list = []\n",
    "    episodelength_list = []\n",
    "    collision_list = []\n",
    "    for i in range(NUM_EVAL_EPISODES):\n",
    "        env.reset()\n",
    "        current_state = env.current_state_id\n",
    "        observation = env.h5_file['resnet_feature'][current_state][0]\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        episode_collision = 0\n",
    "\n",
    "        while env.terminal==False:\n",
    "            action = RL.choose_action(observation)\n",
    "            env.step(action)\n",
    "            observation_ =  env.h5_file['resnet_feature'][env.current_state_id][0]\n",
    "            if env.terminal == True:\n",
    "                R = 10\n",
    "            elif env.collided == True:\n",
    "                R = -0.1\n",
    "                episode_collision += 1\n",
    "            else: \n",
    "                R = -0.01\n",
    "            episode_reward += R\n",
    "            episode_length += 1\n",
    "            observation = observation_\n",
    "        reward_list.append(episode_reward)\n",
    "        episodelength_list.append(episode_length)\n",
    "        collision_list.append(episode_collision)\n",
    "    compare_list.append([reward_list, episodelength_list, collision_list])\n",
    "    #print (\"episodes: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tasks = TASK_LIST\n",
    "scene_scopes = list_of_tasks.keys()\n",
    "\n",
    "\n",
    "branches = []\n",
    "for scene in scene_scopes:\n",
    "    for task in list_of_tasks[scene]:\n",
    "        branches.append((scene, task))\n",
    "\n",
    "scene, task = branches[0]\n",
    "\n",
    "\n",
    "env = Environment({\n",
    "    'scene_name': scene,\n",
    "    'terminal_state_id': int(task)\n",
    "})\n",
    "env.reset()\n",
    "\n",
    "\n",
    "RL = DeepQNetwork(env.nA, 2048,\n",
    "                      learning_rate=0.01,\n",
    "                      reward_decay=0.9,\n",
    "                      e_greedy=0.9,\n",
    "                      replace_target_iter=200,\n",
    "                      memory_size=2000,\n",
    "                      # output_graph=True\n",
    "                      )\n",
    "\n",
    "\n",
    "step = 0\n",
    "compare_list=[]\n",
    "for i in range(1000):\n",
    "    # initial observation\n",
    "    env.reset()\n",
    "    current_state = env.current_state_id\n",
    "    observation = env.h5_file['resnet_feature'][current_state][0]\n",
    "    \n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "        \n",
    "    while env.terminal==False and episode_length<10000:\n",
    "\n",
    "        # RL choose action based on observation\n",
    "        action = RL.choose_action(observation)\n",
    "        # RL take action and get next observation and reward\n",
    "        \n",
    "        env.step(action)\n",
    "        observation_ =  env.h5_file['resnet_feature'][env.current_state_id][0]\n",
    "        if env.terminal == True:\n",
    "            R = 10\n",
    "        elif env.collided == True:\n",
    "            R = -0.1\n",
    "        else: R = -0.01\n",
    "        episode_length += 1\n",
    "        episode_reward += R\n",
    "        RL.store_transition(observation, action, R, observation_)\n",
    "\n",
    "        if (step > 200) and (step % 5 == 0):\n",
    "            RL.learn()\n",
    "\n",
    "        # swap observation\n",
    "        observation = observation_\n",
    "        step += 1\n",
    "    \n",
    "    print (\"episodes: \", i)\n",
    "    print (\"current id: \", env.current_state_id)\n",
    "    print (\"episode reward: \", episode_reward)\n",
    "    print (\"episode length: \", episode_length)\n",
    "    if (i+1) % 200 == 0:\n",
    "        evaluate(compare_list, env, RL)\n",
    "#RL.plot_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RL.plot_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(RL.cost_his)\n",
    "plt.ylim(0,10)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('training steps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(compare_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate reward:  -19.10320000000003\n",
      "evaluate reward:  -22.533700000001367\n",
      "evaluate reward:  -22.728600000001034\n",
      "evaluate reward:  -18.569800000000658\n",
      "evaluate reward:  -58.96659999999918\n",
      "evaluate reward:  -19.742500000000724\n",
      "evaluate reward:  -18.79040000000117\n",
      "evaluate reward:  -20.96000000000171\n",
      "evaluate reward:  -18.777700000000806\n",
      "evaluate reward:  -23.11050000000046\n",
      "evaluate reward:  -19.687500000001275\n",
      "evaluate reward:  -21.737800000000355\n",
      "evaluate reward:  -25.92370000000048\n",
      "evaluate reward:  -11.554100000000322\n",
      "evaluate reward:  -22.521000000000576\n",
      "evaluate reward:  -18.651200000000777\n",
      "evaluate reward:  -21.870700000000603\n",
      "evaluate reward:  -16.77760000000018\n",
      "evaluate reward:  -20.452900000000135\n"
     ]
    }
   ],
   "source": [
    "for i in range(19):\n",
    "    print (\"evaluate reward: \", np.mean(np.array(compare_list)[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print (\"evaluate length: \", np.mean(np.array(compare_list)[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
